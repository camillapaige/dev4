{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2f89af0",
   "metadata": {},
   "source": [
    "Camilla Green \n",
    "\n",
    "5/16/25\n",
    "\n",
    "# Asana Task and Attachment Export Script\n",
    "This script uses the Asana API to access tasks, subtasks, and attachments. The script retrieves tasks and subtasks from all of the projects that the user has access to, extracts their details, and saves them to a CSV file. It also retrieves attachments for each task. The output of the script is saved to the same location as the script itself. The script creates a folder \"attachments\". Each project has a folder inside \"attachments,\" and each task has a folder inside the project folder which contains the associated attachments. The script also creates a log file of any projects that were skipped. For the use case of the GRID3 team, all of the projects that the user has access to will be downloaded. Due to the large file size, projects will be processed in batches of 25. The script zips all of the output into one zip file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d026aa",
   "metadata": {},
   "source": [
    "# To summarize, the output will look like:\n",
    "\n",
    "attachments.zip \n",
    "\n",
    "Inside:\n",
    "\n",
    "/attachments/<project_name>/<task_name>/<attachment_name.extension>\n",
    "\n",
    "The csv is named <project_name>_export.csv, is inside <project_name> folder, and contains fields for all of the tasks and subtasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6765b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary packages\n",
    "import requests\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import shutil\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9f26c0",
   "metadata": {},
   "source": [
    "In my case, I saved my API key locally. The script reads the API key from this file. Instead, you can replace the API key in the following chunk with your own API key\n",
    "\n",
    "# First get your asana pat from here:\n",
    "https://app.asana.com/0/my-apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111d831c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'gid': '1203996791156052', 'email': 'cgreen@ciesin.columbia.edu', 'name': 'Camilla Green', 'photo': {'image_21x21': 'https://s3.us-east-1.amazonaws.com/asana-user-private-us-east-1/assets/448138111837173/profile_photos/1204066490840113/624697e7c14722bc376d262d72164e23_21x21.png', 'image_27x27': 'https://s3.us-east-1.amazonaws.com/asana-user-private-us-east-1/assets/448138111837173/profile_photos/1204066490840113/624697e7c14722bc376d262d72164e23_27x27.png', 'image_36x36': 'https://s3.us-east-1.amazonaws.com/asana-user-private-us-east-1/assets/448138111837173/profile_photos/1204066490840113/624697e7c14722bc376d262d72164e23_36x36.png', 'image_60x60': 'https://s3.us-east-1.amazonaws.com/asana-user-private-us-east-1/assets/448138111837173/profile_photos/1204066490840113/624697e7c14722bc376d262d72164e23_60x60.png', 'image_128x128': 'https://s3.us-east-1.amazonaws.com/asana-user-private-us-east-1/assets/448138111837173/profile_photos/1204066490840113/624697e7c14722bc376d262d72164e23_128x128.png'}, 'resource_type': 'user', 'workspaces': [{'gid': '448138111837173', 'name': 'ciesin.columbia.edu', 'resource_type': 'workspace'}]}}\n"
     ]
    }
   ],
   "source": [
    "#pat - personal access token\n",
    "#these lines are commented out because they access my API key from the environment. Instead enter your own token below\n",
    "#load_dotenv() \n",
    "#PAT = os.getenv(\"PAT\")\n",
    "\n",
    "#REPLACE THIS WITH YOUR OWN TOKEN\n",
    "PAT = \"YOUR OWN TOKEN HERE\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {PAT}\"\n",
    "}\n",
    "#print info about owner of api token\n",
    "response = requests.get(\"https://app.asana.com/api/1.0/users/me\", headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4b56ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this will determine if the user is authenticated and print the user if it is successful\n",
    "def verify_authentication():\n",
    "    response = requests.get(\"https://app.asana.com/api/1.0/users/me\", headers=headers)\n",
    "    user = response.json().get(\"data\", {}).get(\"name\", \"Unknown\")\n",
    "    print(f\"üîê Authenticated as: {user}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6fac294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîê Authenticated as: Camilla Green\n"
     ]
    }
   ],
   "source": [
    "verify_authentication()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a1fefa",
   "metadata": {},
   "source": [
    "This will list all the asana projects so you can get the relevant project id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa32b757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': [{'gid': '1147666751701275', 'name': 'Cleaning and processing basemap layers from Haut-Lomami and Tanganyika', 'resource_type': 'project'}, {'gid': '1148360962805936', 'name': 'Production and Delivery of Boundaries (Alpha)', 'resource_type': 'project'}, {'gid': '1153037761383214', 'name': 'Production and Delivery of Boundaries (Beta)', 'resource_type': 'project'}, {'gid': '1148360962805943', 'name': 'Delivery of Settlement layer (microplan, comprehensive and various versions) (Alpha)', 'resource_type': 'project'}, {'gid': '1148360962805948', 'name': 'Delivery of FOSA layer (Alpha)', 'resource_type': 'project'}, {'gid': '1148949750639595', 'name': 'Kwilu Use Case Workshop', 'resource_type': 'project'}, {'gid': '1149607162606753', 'name': 'Weekly Check-in ', 'resource_type': 'project'}, {'gid': '1158312676770821', 'name': 'Settlement base maps', 'resource_type': 'project'}, {'gid': '1158312676770825', 'name': 'Infrastructure maps', 'resource_type': 'project'}, {'gid': '1162922621378241', 'name': 'Country-level suitability assessments', 'resource_type': 'project'}, {'gid': '1162922621378246', 'name': 'Country-level readiness assessments', 'resource_type': 'project'}, {'gid': '1162922621378250', 'name': 'Country-level plans', 'resource_type': 'project'}, {'gid': '1162922621378254', 'name': 'Country-level resource mobilization efforts', 'resource_type': 'project'}, {'gid': '1162922621378258', 'name': 'Regional workshops', 'resource_type': 'project'}, {'gid': '1162922621378266', 'name': 'Subnational administrative boundary data sets', 'resource_type': 'project'}, {'gid': '1162922621378270', 'name': 'White papers focused on GRID3 products ', 'resource_type': 'project'}, {'gid': '1162922621378278', 'name': 'Hybrid census population data sets', 'resource_type': 'project'}, {'gid': '1162930454038737', 'name': 'Geospatial census modernization support', 'resource_type': 'project'}, {'gid': '1162922621332242', 'name': 'On-site courses', 'resource_type': 'project'}, {'gid': '1158312676770831', 'name': 'Distance learning courses', 'resource_type': 'project'}, {'gid': '1162930454038741', 'name': 'Workshop and training curricula', 'resource_type': 'project'}, {'gid': '1162930454038749', 'name': 'Outreach events ', 'resource_type': 'project'}, {'gid': '1162930454038753', 'name': 'Global Advisory Board Meetings', 'resource_type': 'project'}, {'gid': '1162930454038757', 'name': 'Global Technical Advisory Panel meetings', 'resource_type': 'project'}, {'gid': '1162930454038761', 'name': 'Targeted web content based on white papers', 'resource_type': 'project'}, {'gid': '1162930454038765', 'name': 'Targeted social media campaigns', 'resource_type': 'project'}, {'gid': '1162930454038769', 'name': 'GRID3 newsletters', 'resource_type': 'project'}, {'gid': '1169947759028822', 'name': 'Website success stories', 'resource_type': 'project'}, {'gid': '1162930454038781', 'name': 'M&E framework and methodology', 'resource_type': 'project'}, {'gid': '1162930454038785', 'name': 'M&E assessments', 'resource_type': 'project'}, {'gid': '1162930454038789', 'name': 'Resource mobilization strategy', 'resource_type': 'project'}, {'gid': '1162930454038793', 'name': 'Identification of major fundraising prospects', 'resource_type': 'project'}, {'gid': '1162930454038797', 'name': 'High-level investment pitches to new funding prospects', 'resource_type': 'project'}, {'gid': '1162930454038801', 'name': 'Donor workshops', 'resource_type': 'project'}, {'gid': '1165990642594033', 'name': 'DRC visuals / maps creation for presentation', 'resource_type': 'project'}, {'gid': '1166220762911385', 'name': 'Delivery of HL/TN Maps ', 'resource_type': 'project'}, {'gid': '1169950795343825', 'name': 'Settlement extents', 'resource_type': 'project'}, {'gid': '1169950795387838', 'name': 'Other population data sets', 'resource_type': 'project'}, {'gid': '1169950795387850', 'name': 'Settlement base maps (phase 1)', 'resource_type': 'project'}, {'gid': '1169950795387860', 'name': 'Infrastructure maps (phase 1)', 'resource_type': 'project'}, {'gid': '1172631986120625', 'name': 'Esri tasks', 'resource_type': 'project'}, {'gid': '1172631986120628', 'name': 'Fraym tasks', 'resource_type': 'project'}, {'gid': '1174078588420409', 'name': 'Oversight of all WkPs', 'resource_type': 'project'}, {'gid': '1174078588420543', 'name': 'Meetings', 'resource_type': 'project'}, {'gid': '1174113120309602', 'name': 'WkP1. Basemaps production', 'resource_type': 'project'}, {'gid': '1174113120309622', 'name': 'WkP 1. R&I log', 'resource_type': 'project'}, {'gid': '1174113120309631', 'name': 'WkP 2 - Population estimates', 'resource_type': 'project'}, {'gid': '1174113120309653', 'name': 'WkP2. R&I log', 'resource_type': 'project'}, {'gid': '1174113120309671', 'name': 'WkP 3 - Mobility Estimates', 'resource_type': 'project'}, {'gid': '1174121110870732', 'name': 'WkP3. R&I log', 'resource_type': 'project'}, {'gid': '1174121110870747', 'name': 'WkP4 Engagement', 'resource_type': 'project'}, {'gid': '1174121110870768', 'name': 'WkP4. R&I log', 'resource_type': 'project'}, {'gid': '1174121110870799', 'name': 'WkP 5 - Gender', 'resource_type': 'project'}, {'gid': '1174121110870811', 'name': 'WkP5. R&I log', 'resource_type': 'project'}, {'gid': '1174121110870836', 'name': 'WkP6 Micro-plans improvement', 'resource_type': 'project'}, {'gid': '1174121110870850', 'name': 'WkP6. R&I log', 'resource_type': 'project'}, {'gid': '1174121110870866', 'name': 'WkP7 Capacity-strengthening and sustainability', 'resource_type': 'project'}, {'gid': '1174121110870884', 'name': 'WkP7. R&I log', 'resource_type': 'project'}, {'gid': '1174099808056139', 'name': 'WkP 8 - Coordination', 'resource_type': 'project'}, {'gid': '1174099808056188', 'name': 'WkP8. R&I log', 'resource_type': 'project'}, {'gid': '1177904325025733', 'name': 'Jolynn Schmidt', 'resource_type': 'project'}, {'gid': '1198136803644204', 'name': 'Sierra Leone Coordination', 'resource_type': 'project'}, {'gid': '1200088150113409', 'name': '2021 Country-level suitability assessments', 'resource_type': 'project'}, {'gid': '1200088150113411', 'name': '2021 Country-level readiness assessments', 'resource_type': 'project'}, {'gid': '1200088150113413', 'name': '2021 Country-level plans', 'resource_type': 'project'}, {'gid': '1200088150113415', 'name': '2021 Country-level resource mobilization efforts', 'resource_type': 'project'}, {'gid': '1200088150113422', 'name': '2021 Infrastructure maps', 'resource_type': 'project'}, {'gid': '1200088150113428', 'name': '2021 Hybrid census population data sets', 'resource_type': 'project'}, {'gid': '1200088150113432', 'name': '2021 Other population data sets', 'resource_type': 'project'}, {'gid': '1200088150113407', 'name': '2021 Settlement base maps', 'resource_type': 'project'}, {'gid': '1200088150113424', 'name': '2021  Boundary datasets', 'resource_type': 'project'}, {'gid': '1200088150113426', 'name': '2021 White papers focused on GRID3 products ', 'resource_type': 'project'}, {'gid': '1200088150113434', 'name': '2021 High priority spatial datasets', 'resource_type': 'project'}, {'gid': '1200088150113456', 'name': '2021 Country routine population mobility analysis', 'resource_type': 'project'}, {'gid': '1200088150607383', 'name': '2021 Geospatial census modernization support', 'resource_type': 'project'}, {'gid': '1200088150753431', 'name': '2021 Distance learning courses', 'resource_type': 'project'}, {'gid': '1200088150753433', 'name': '2021 Workshop and training curricula', 'resource_type': 'project'}, {'gid': '1200088150753455', 'name': '2021 Enhanced COVID data infrastructure', 'resource_type': 'project'}, {'gid': '1200088150753457', 'name': '2021 Advocacy and Outreach events ', 'resource_type': 'project'}, {'gid': '1200088150113417', 'name': '2021 Regional workshops', 'resource_type': 'project'}, {'gid': '1200088150753459', 'name': '2021 Global Advisory Board Meetings', 'resource_type': 'project'}, {'gid': '1200088150753461', 'name': '2021 Global Technical Advisory Panel meetings', 'resource_type': 'project'}, {'gid': '1200088150753465', 'name': '2021 White papers based targeted web content', 'resource_type': 'project'}, {'gid': '1200088150753471', 'name': '2021 Website success stories', 'resource_type': 'project'}, {'gid': '1200088150753467', 'name': '2021 Targeted social media campaigns', 'resource_type': 'project'}, {'gid': '1200088150753469', 'name': '2021 GRID3 newsletters', 'resource_type': 'project'}, {'gid': '1200088150753531', 'name': '2021 M&E framework and methodology', 'resource_type': 'project'}, {'gid': '1200088150753533', 'name': '2021 M&E assessments', 'resource_type': 'project'}, {'gid': '1200088150753537', 'name': '2021 Identification of major fundraising prospects', 'resource_type': 'project'}, {'gid': '1200088150753539', 'name': '2021 High-level investment pitches to new funding prospects', 'resource_type': 'project'}, {'gid': '1200088150753541', 'name': '2021 Donor workshops', 'resource_type': 'project'}, {'gid': '1200088150753545', 'name': '2021 Partner learning & strategy workshop', 'resource_type': 'project'}, {'gid': '1204001206636467', 'name': 'GPWv5 Meetings', 'resource_type': 'project'}, {'gid': '1204167517012441', 'name': 'Sprint 23.2 - Apr. 21 - May 19 - Growth Rates & Script updates', 'resource_type': 'project'}, {'gid': '1203996375607544', 'name': 'Sprint 23.1 - Feb. 24 - Mar. 17 - Boundary Ingest & Matching', 'resource_type': 'project'}, {'gid': '1204321402243183', 'name': 'GPWv5 Statistical Areas', 'resource_type': 'project'}, {'gid': '1204856707917453', 'name': 'GPWv5 Development Workflow', 'resource_type': 'project'}, {'gid': '1204871602075010', 'name': 'Ad Hoc Tasks, Ideas, Errata, or Problems to Resolve', 'resource_type': 'project'}, {'gid': '1205268902980264', 'name': 'Custom Fields üìç Location', 'resource_type': 'project'}, {'gid': '1205336296764514', 'name': 'GPWv5 Release 1', 'resource_type': 'project'}, {'gid': '1205387316271786', 'name': 'TOPSTSCHOOL Main Project', 'resource_type': 'project'}, {'gid': '1205387316271789', 'name': 'SCHOOL MODULE 1 Water Resources', 'resource_type': 'project'}, {'gid': '1205731634352654', 'name': 'Archive GPWv5 Development Workflow', 'resource_type': 'project'}, {'gid': '1205919838078368', 'name': 'SCHOOL MODULES 2-4 Workplan', 'resource_type': 'project'}, {'gid': '1206395819210195', 'name': 'Weekly Team Meetings üó£Ô∏èüìù‚úÖ ', 'resource_type': 'project'}, {'gid': '1206412020210390', 'name': 'Asana Conventions', 'resource_type': 'project'}, {'gid': '1206525946402063', 'name': 'Draft Project Template for SCHOOL Modules', 'resource_type': 'project'}, {'gid': '1207249395389700', 'name': 'SCHOOL Module 3 Natural Disasters', 'resource_type': 'project'}, {'gid': '1207362866702993', 'name': \"Julianna Desjardins's previously assigned tasks\", 'resource_type': 'project'}, {'gid': '1207425375557957', 'name': 'Intern Project DRAFT Template', 'resource_type': 'project'}, {'gid': '1207425379981791', 'name': 'Intern Task Backlog', 'resource_type': 'project'}, {'gid': '1207561596445372', 'name': 'Intern management ', 'resource_type': 'project'}, {'gid': '1207577334102514', 'name': 'CIESIN GPW Asana Conventions', 'resource_type': 'project'}, {'gid': '1207590366056662', 'name': \"Sofie's GPW Project\", 'resource_type': 'project'}, {'gid': '1207591405580759', 'name': \"Ananya's GPW Project\", 'resource_type': 'project'}, {'gid': '1207590366056682', 'name': \"Carmelli's GPW Project\", 'resource_type': 'project'}, {'gid': '1207591405580764', 'name': \"Madeline's GPW Project\", 'resource_type': 'project'}, {'gid': '1207592283474960', 'name': \"Ljupcho's GPW Project\", 'resource_type': 'project'}, {'gid': '1208611132049663', 'name': \"Ryan's GPW Project\", 'resource_type': 'project'}]}\n"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {PAT}\"\n",
    "}\n",
    "\n",
    "response = requests.get(\"https://app.asana.com/api/1.0/projects\", headers=headers)\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75f36a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Scanning projects in workspace: ciesin.columbia.edu\n",
      "\n",
      " Total projects added: 116\n"
     ]
    }
   ],
   "source": [
    "#this will get info on all projects accessible by the authenticated user\n",
    "def get_all_accessible_projects():\n",
    "    projects = {}\n",
    "    # First get workspaces\n",
    "    workspaces_url = \"https://app.asana.com/api/1.0/workspaces\"\n",
    "    workspaces_response = requests.get(workspaces_url, headers=headers)\n",
    "    workspaces = workspaces_response.json().get(\"data\", [])\n",
    "\n",
    "    # Then get projects in each workspace\n",
    "    for ws in workspaces:\n",
    "        workspace_gid = ws[\"gid\"]\n",
    "        workspace_name = ws[\"name\"]\n",
    "        print(f\"üîç Scanning projects in workspace: {workspace_name}\")\n",
    "        projects_url = f\"https://app.asana.com/api/1.0/projects?workspace={workspace_gid}&archived=false\"\n",
    "        while projects_url:\n",
    "            proj_response = requests.get(projects_url, headers=headers)\n",
    "            proj_data = proj_response.json().get(\"data\", [])\n",
    "            for p in proj_data:\n",
    "                projects[p[\"name\"]] = p[\"gid\"]\n",
    "            projects_url = proj_response.json().get(\"next_page\", {}).get(\"uri\")\n",
    "    \n",
    "    \n",
    "    print(f\"\\n Total projects added: {len(projects)}\")\n",
    "    return projects\n",
    "\n",
    "\n",
    "# replace hardcoded dictionary:\n",
    "projects = get_all_accessible_projects()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "899c6905",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create batches of 25 projects to process at a time\n",
    "def batch_projects(projects_dict, batch_size=25):\n",
    "    items = list(projects_dict.items())\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        yield dict(items[i:i + batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84bb543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Functions\n",
    "# get project name from Asana API using its GID - this is used to create a folder for the project\n",
    "def get_project_name(project_gid):\n",
    "    \"\"\"Get the project name from Asana API using its GID.\"\"\"\n",
    "    url = f\"https://app.asana.com/api/1.0/projects/{project_gid}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    return response.json()[\"data\"].get(\"name\", \"asana_project\")\n",
    "\n",
    "#clean the name of the project to make it a valid folder name\n",
    "#this is done by replacing invalid characters with underscores\n",
    "def clean_filename(name):\n",
    "    \"\"\"Sanitize file and folder names.\"\"\"\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", name.replace(\" \", \"_\"))\n",
    "\n",
    "#create a folder for the project with the project name from asana,\n",
    "#this folder will go inside the folder named \"attachments\" \n",
    "def create_output_folder(project_gid, base_path=\"attachments\"):\n",
    "    project_name = clean_filename(get_project_name(project_gid))\n",
    "    folder_name = f\"{project_name}\"\n",
    "    output_path = os.path.join(base_path, folder_name)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    return output_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cfabe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task and Metadata Export\n",
    "#get all tasks for a project\n",
    "def get_all_tasks(project_gid):\n",
    "    tasks = []\n",
    "    url = f\"https://app.asana.com/api/1.0/projects/{project_gid}/tasks\"\n",
    "    params = {\"opt_fields\": \"name,completed,assignee.name,due_on,notes,custom_fields\"}\n",
    "    while url:\n",
    "        r = requests.get(url, headers=headers, params=params)\n",
    "        tasks.extend(r.json().get(\"data\", []))\n",
    "        url = r.json().get(\"next_page\", {}).get(\"uri\")\n",
    "    return tasks\n",
    "\n",
    "#get all subtasks for each task\n",
    "def get_subtasks_for_task(task_gid):\n",
    "    subtasks = []\n",
    "    url = f\"https://app.asana.com/api/1.0/tasks/{task_gid}/subtasks\"\n",
    "    params = {\"opt_fields\": \"name,completed\"}\n",
    "    while url:\n",
    "        r = requests.get(url, headers=headers, params=params)\n",
    "        subtasks.extend(r.json().get(\"data\", []))\n",
    "        url = r.json().get(\"next_page\", {}).get(\"uri\")\n",
    "    return subtasks\n",
    "\n",
    "#get stories (comments and system history/events) for each task and subtask\n",
    "def get_stories_for_task(task_gid):\n",
    "    stories = []\n",
    "    url = f\"https://app.asana.com/api/1.0/tasks/{task_gid}/stories\"\n",
    "    while url:\n",
    "        r = requests.get(url, headers=headers)\n",
    "        for s in r.json().get(\"data\", []):\n",
    "            created_by = s.get(\"created_by\")\n",
    "            author = created_by[\"name\"] if created_by else \"System\"\n",
    "            stories.append({\n",
    "                \"type\": s.get(\"type\"),\n",
    "                \"resource_subtype\": s.get(\"resource_subtype\"),\n",
    "                \"author\": author,\n",
    "                \"created_at\": s.get(\"created_at\"),\n",
    "                \"text\": s.get(\"text\", \"\")\n",
    "            })\n",
    "        url = r.json().get(\"next_page\", {}).get(\"uri\")\n",
    "    return stories\n",
    "\n",
    "#format stories (comments) into a single string so they can be written to a csv as 1 field\n",
    "def flatten_stories(stories):\n",
    "    return \" | \".join(\n",
    "        f\"[{s['created_at']}] {s['author']} ({s['resource_subtype']}): {s['text']}\"\n",
    "        for s in stories\n",
    "    )\n",
    "\n",
    "#format subtasks into a single string so they can be written to a csv as 1 field\n",
    "def flatten_subtasks(subtasks):\n",
    "    lines = []\n",
    "    for s in subtasks:\n",
    "        name = s['name']\n",
    "        completed = s['completed']\n",
    "        stories = get_stories_for_task(s['gid'])\n",
    "        story_text = flatten_stories(stories)\n",
    "        lines.append(f\"{name} (Completed: {completed}) - Stories: [{story_text}]\")\n",
    "    return \" | \".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97c2051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export the project data to a csv file\n",
    "def export_project_data_to_csv(project_gid, output_folder, log_file=\"skipped_projects.log\"):\n",
    "    \"\"\"Export full task data to a CSV file or log if no tasks found.\"\"\"\n",
    "    project_name = clean_filename(get_project_name(project_gid))\n",
    "    output_file = os.path.join(output_folder, f\"{project_name}_export.csv\")\n",
    "    tasks = get_all_tasks(project_gid)\n",
    "    all_task_data = []\n",
    "\n",
    "    for task in tasks:\n",
    "        subtasks = get_subtasks_for_task(task[\"gid\"])\n",
    "        stories = get_stories_for_task(task[\"gid\"])\n",
    "        task_data = {\n",
    "            \"Task GID\": task[\"gid\"],\n",
    "            \"Name\": task.get(\"name\", \"[No name]\"),\n",
    "            \"Completed\": task.get(\"completed\"),\n",
    "            \"Assignee\": task.get(\"assignee\", {}).get(\"name\", \"Unassigned\") if task.get(\"assignee\") else \"Unassigned\",\n",
    "            \"Due On\": task.get(\"due_on\") or \"No due date\",\n",
    "            \"Notes\": task.get(\"notes\") or \"No notes\",\n",
    "            \"Custom Fields\": \", \".join([\n",
    "                f\"{f.get('name')}: {f.get('text_value') or f.get('number_value') or (f.get('enum_value') or {}).get('name', '')}\"\n",
    "                for f in task.get(\"custom_fields\", []) if f\n",
    "            ]),\n",
    "            \"Subtasks\": flatten_subtasks(subtasks),\n",
    "            \"Stories\": flatten_stories(stories)\n",
    "        }\n",
    "        all_task_data.append(task_data)\n",
    "\n",
    "    if not all_task_data:\n",
    "        msg = f\"{project_name} (GID: {project_gid}) - Skipped: No tasks found\\n\"\n",
    "        with open(log_file, \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(msg)\n",
    "        print(f\"‚ö†Ô∏è {msg.strip()}\")\n",
    "        return\n",
    "\n",
    "    with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=all_task_data[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_task_data)\n",
    "\n",
    "    print(f\"üìÑ Exported {len(all_task_data)} tasks to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a36d017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access attachments for each project, process each task and download into folders based on task name\n",
    "def get_attachments_for_task(task_gid):\n",
    "    attachments = []\n",
    "    url = f\"https://app.asana.com/api/1.0/tasks/{task_gid}/attachments\"\n",
    "    while url:\n",
    "        r = requests.get(url, headers=headers)\n",
    "        attachments.extend(r.json().get(\"data\", []))\n",
    "        url = r.json().get(\"next_page\", {}).get(\"uri\")\n",
    "    return attachments\n",
    "\n",
    "#get the download URL for each attachement\n",
    "def get_download_url(attachment_gid):\n",
    "    url = f\"https://app.asana.com/api/1.0/attachments/{attachment_gid}\"\n",
    "    r = requests.get(url, headers=headers)\n",
    "    return r.json().get(\"data\", {}).get(\"download_url\") if r.status_code == 200 else None\n",
    "\n",
    "# helper function to safely download with retries\n",
    "def safe_download(url, retries=3, timeout=30):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, stream=True, timeout=timeout)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Status {response.status_code} for {url}\")\n",
    "                return None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"‚ö†Ô∏è Attempt {attempt + 1} failed: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(2 ** attempt)  # exponential backoff\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "# Main attachment download function\n",
    "def download_all_attachments(project_gid, base_folder, log_file=\"skipped_projects.log\"):\n",
    "    tasks = get_all_tasks(project_gid)\n",
    "\n",
    "    if not tasks:\n",
    "        project_name = clean_filename(get_project_name(project_gid))\n",
    "        msg = f\"{project_name} (GID: {project_gid}) - Skipped: No tasks for attachment download\\n\"\n",
    "        with open(log_file, \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(msg)\n",
    "        print(f\"‚ö†Ô∏è {msg.strip()}\")\n",
    "        return\n",
    "\n",
    "    for task in tasks:\n",
    "        task_gid = task[\"gid\"]\n",
    "        task_name = clean_filename(task.get(\"name\", \"untitled_task\"))\n",
    "        task_folder = os.path.join(base_folder, f\"{task_name}\")\n",
    "        os.makedirs(task_folder, exist_ok=True)\n",
    "\n",
    "        attachments = get_attachments_for_task(task_gid)\n",
    "        for att in attachments:\n",
    "            url = get_download_url(att[\"gid\"]) or att.get(\"permalink_url\")\n",
    "            if not url:\n",
    "                print(f\"‚ö†Ô∏è Skipping {att['name']}: no download URL\")\n",
    "                continue\n",
    "            try:\n",
    "                print(f\"‚¨áÔ∏è Downloading {att['name']} to {task_folder}...\")\n",
    "                r = safe_download(url)\n",
    "                if r:\n",
    "                    file_path = os.path.join(task_folder, clean_filename(att[\"name\"]))\n",
    "                    with open(file_path, \"wb\") as f:\n",
    "                        for chunk in r.iter_content(1024):\n",
    "                            f.write(chunk)\n",
    "                    print(f\"‚úÖ Saved to {file_path}\")\n",
    "                else:\n",
    "                    print(f\"‚ùå Failed to download {att['name']} after retries\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error downloading {att['name']}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a2a8e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this will zip the folder containing all the attachments into \"attachments.zip\" \n",
    "def zip_folder(folder_path, zip_name=None, delete_original=True):\n",
    "    #If you want to keep the original folder after zipping, set delete_original=False.\n",
    "    if not zip_name:\n",
    "        zip_name = folder_path + \".zip\"\n",
    "\n",
    "    print(f\"üóúÔ∏è Zipping folder '{folder_path}' to '{zip_name}'...\")\n",
    "    with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED, allowZip64=True) as zipf:\n",
    "        for root, _, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, start=folder_path)\n",
    "                try:\n",
    "                    with open(file_path, 'rb') as fsrc:\n",
    "                        zipf.writestr(arcname, fsrc.read())  # reads small chunks internally\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Skipped {file_path} due to error: {e}\")\n",
    "\n",
    "    print(f\"‚úÖ Zip created: {zip_name}\")\n",
    "\n",
    "    if delete_original:\n",
    "        shutil.rmtree(folder_path)\n",
    "        print(f\"üßπ Deleted original folder: {folder_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83ff3fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop to process tasks, subtasks, and attachments\n",
    "# this processes all projects in the dictionary in batches and creates a folder for each project\n",
    "# zip the attachments folder at the end\n",
    "def process_all_projects(projects, combined_output_root=\"attachments\", combined_zip_name=\"attachments.zip\"):\n",
    "    os.makedirs(combined_output_root, exist_ok=True)\n",
    "    verify_authentication() \n",
    "    for batch_num, project_batch in enumerate(batch_projects(projects, batch_size=25), start=1):\n",
    "        print(f\"\\nüì¶ Starting Batch {batch_num} (processing {len(project_batch)} projects)...\")\n",
    "        for name, gid in project_batch.items():\n",
    "            print(f\"\\nProcessing Project: {name}\")\n",
    "            project_folder = create_output_folder(gid, base_path=combined_output_root)\n",
    "            export_project_data_to_csv(gid, project_folder)\n",
    "            download_all_attachments(gid, project_folder)\n",
    "\n",
    "    # create one zip from the full attachments directory\n",
    "    zip_folder(combined_output_root, combined_zip_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a225a64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîê Authenticated as: Camilla Green\n",
      "\n",
      "üì¶ Starting Batch 1 (processing 25 projects)...\n",
      "\n",
      "Processing Project: Cleaning and processing basemap layers from Haut-Lomami and Tanganyika\n",
      "üìÑ Exported 1 tasks to 'attachments/Cleaning_and_processing_basemap_layers_from_Haut-Lomami_and_Tanganyika/Cleaning_and_processing_basemap_layers_from_Haut-Lomami_and_Tanganyika_export.csv'\n",
      "\n",
      "Processing Project: Production and Delivery of Boundaries (Alpha)\n",
      "üìÑ Exported 8 tasks to 'attachments/Production_and_Delivery_of_Boundaries_(Alpha)/Production_and_Delivery_of_Boundaries_(Alpha)_export.csv'\n",
      "\n",
      "Processing Project: Production and Delivery of Boundaries (Beta)\n",
      "üìÑ Exported 21 tasks to 'attachments/Production_and_Delivery_of_Boundaries_(Beta)/Production_and_Delivery_of_Boundaries_(Beta)_export.csv'\n",
      "‚¨áÔ∏è Downloading image.png to attachments/Production_and_Delivery_of_Boundaries_(Beta)/Issues_in_Kayamba...\n",
      "‚úÖ Saved to attachments/Production_and_Delivery_of_Boundaries_(Beta)/Issues_in_Kayamba/image.png\n",
      "‚¨áÔ∏è Downloading Capture_Kayi_20191128.JPG to attachments/Production_and_Delivery_of_Boundaries_(Beta)/Issues_in_Kayamba...\n",
      "‚úÖ Saved to attachments/Production_and_Delivery_of_Boundaries_(Beta)/Issues_in_Kayamba/Capture_Kayi_20191128.JPG\n",
      "‚¨áÔ∏è Downloading image.png to attachments/Production_and_Delivery_of_Boundaries_(Beta)/Issues_in_Songa...\n",
      "‚úÖ Saved to attachments/Production_and_Delivery_of_Boundaries_(Beta)/Issues_in_Songa/image.png\n",
      "‚¨áÔ∏è Downloading Sogna_geopode_poisettlements_vs_localitecleaned.PNG to attachments/Production_and_Delivery_of_Boundaries_(Beta)/Issues_in_Songa...\n",
      "‚úÖ Saved to attachments/Production_and_Delivery_of_Boundaries_(Beta)/Issues_in_Songa/Sogna_geopode_poisettlements_vs_localitecleaned.PNG\n",
      "‚¨áÔ∏è Downloading KitengeKabalo_gap.PNG to attachments/Production_and_Delivery_of_Boundaries_(Beta)/Revisit_where_Bekisha,_Lubonge,_Kaongo,_and_Kayambi_AS_meet._There_was_a_large_gap_that_I_fixed_based_on_geopode_data._I_would_like_confirmation_it_is_a_good_fix._See_description....\n",
      "‚úÖ Saved to attachments/Production_and_Delivery_of_Boundaries_(Beta)/Revisit_where_Bekisha,_Lubonge,_Kaongo,_and_Kayambi_AS_meet._There_was_a_large_gap_that_I_fixed_based_on_geopode_data._I_would_like_confirmation_it_is_a_good_fix._See_description./KitengeKabalo_gap.PNG\n",
      "‚¨áÔ∏è Downloading Kinda_KabondoDianda.PNG to attachments/Production_and_Delivery_of_Boundaries_(Beta)/Boundary_of_AS_Kamshipa_(Kinda)_and_AS_Uzima_(Kabondo_Dianda)_may_need_adjusting._Geopode_points_in_Kamishipa_have_AS_Uzima_which_is_in_Kabondo_Dianda,_but_comments_indicate_village_is_in_Kinda._Confusion.)...\n",
      "‚úÖ Saved to attachments/Production_and_Delivery_of_Boundaries_(Beta)/Boundary_of_AS_Kamshipa_(Kinda)_and_AS_Uzima_(Kabondo_Dianda)_may_need_adjusting._Geopode_points_in_Kamishipa_have_AS_Uzima_which_is_in_Kabondo_Dianda,_but_comments_indicate_village_is_in_Kinda._Confusion.)/Kinda_KabondoDianda.PNG\n",
      "‚¨áÔ∏è Downloading Kinda_KabondoDianda_geopodeKD_cleaned.PNG to attachments/Production_and_Delivery_of_Boundaries_(Beta)/Boundary_of_AS_Kamshipa_(Kinda)_and_AS_Uzima_(Kabondo_Dianda)_may_need_adjusting._Geopode_points_in_Kamishipa_have_AS_Uzima_which_is_in_Kabondo_Dianda,_but_comments_indicate_village_is_in_Kinda._Confusion.)...\n",
      "‚úÖ Saved to attachments/Production_and_Delivery_of_Boundaries_(Beta)/Boundary_of_AS_Kamshipa_(Kinda)_and_AS_Uzima_(Kabondo_Dianda)_may_need_adjusting._Geopode_points_in_Kamishipa_have_AS_Uzima_which_is_in_Kabondo_Dianda,_but_comments_indicate_village_is_in_Kinda._Confusion.)/Kinda_KabondoDianda_geopodeKD_cleaned.PNG\n",
      "‚¨áÔ∏è Downloading Manyanga_LamboKatenga_Nyemba.PNG to attachments/Production_and_Delivery_of_Boundaries_(Beta)/One_point_for_village_Manyanga_north_of_the_boundary_of_AS_Lambo_Katenga_(Nyemba)_that_is_noted_as_being_in_Lambo_Katenga...\n"
     ]
    }
   ],
   "source": [
    "# Run everything\n",
    "#this will process all the projects listed in the dictionary, providing a csv file and a folder with attachments for each project\n",
    "#the csv file will be named <project_name>_export.csv and have all the tasks, subtasks, and their metadata\n",
    "#the attachments will be downloaded into folders named after the tasks\n",
    "process_all_projects(projects)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asana-mig",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
